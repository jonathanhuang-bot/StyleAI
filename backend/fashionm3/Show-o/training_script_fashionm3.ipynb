{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb9b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "#from lightning.pytorch.utilities import CombinedLoader\n",
    "from pytorch_lightning.utilities import CombinedLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import DistributedType, set_seed\n",
    "\n",
    "#from training.data import Text2ImageDataset\n",
    "#from training.imagenet_dataset import ImageNetDataset\n",
    "#from parquet import RefinedWebDataset\n",
    "from fashionrec_dataset import FashionRecDataset\n",
    "from fashion_image_generation_dataset import FashionImageGenerationDataset\n",
    "\n",
    "from models import Showo, MAGVITv2, get_mask_chedule\n",
    "from training.prompting_utils import UniversalPrompting, create_attention_mask_predict_next, create_attention_mask_for_mmu\n",
    "from models.lr_schedulers import get_scheduler\n",
    "from models.logging import set_verbosity_info, set_verbosity_error\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from llava.llava_data_vq_unified import get_instruct_data_loader\n",
    "from training.utils import get_config, flatten_omega_conf, mask_or_random_replace_tokens, AverageMeter\n",
    "\n",
    "SYSTEM_PROMPT_LEN = 28\n",
    "logger = get_logger(__name__, log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e766674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batch size: 6\n",
      "Per GPU batch size: 6\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config = OmegaConf.load('configs/fashionm3_training.yaml')\n",
    "\n",
    "# Enable TF32 on Ampere GPUs\n",
    "if config.training.enable_tf32:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "config.experiment.logging_dir = str(Path(config.experiment.output_dir) / \"logs\")\n",
    "\n",
    "# Initialize accelerator\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
    "    mixed_precision=config.training.mixed_precision,\n",
    "    log_with=\"wandb\",\n",
    "    project_dir=config.experiment.logging_dir,\n",
    "    split_batches=True,\n",
    ")\n",
    "\n",
    "# Calculate batch sizes\n",
    "total_batch_size_per_gpu = (config.training.batch_size_t2i +\n",
    "config.training.batch_size_mmu)\n",
    "total_batch_size = (\n",
    "    (config.training.batch_size_t2i + config.training.batch_size_mmu)\n",
    "    * accelerator.num_processes * config.training.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(f\"Total batch size: {total_batch_size}\")\n",
    "print(f\"Per GPU batch size: {total_batch_size_per_gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b1e47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/04/2025 13:41:59 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonathanhuang781\u001b[0m (\u001b[33muhhidk\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\wandb\\run-20250804_134159-9takx3ze</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uhhidk/fashionm3/runs/9takx3ze' target=\"_blank\">fashionvlm-finetuning</a></strong> to <a href='https://wandb.ai/uhhidk/fashionm3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uhhidk/fashionm3' target=\"_blank\">https://wandb.ai/uhhidk/fashionm3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uhhidk/fashionm3/runs/9takx3ze' target=\"_blank\">https://wandb.ai/uhhidk/fashionm3/runs/9takx3ze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    set_verbosity_info()\n",
    "else:\n",
    "    set_verbosity_error()\n",
    "\n",
    "# Initialize experiment tracking\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(config.experiment.output_dir, exist_ok=True)\n",
    "    OmegaConf.save(config, Path(config.experiment.output_dir) / \"config.yaml\")\n",
    "\n",
    "    # Convert OmegaConf to dict for wandb\n",
    "    config_dict = OmegaConf.to_container(config, resolve=True)\n",
    "\n",
    "    accelerator.init_trackers(\n",
    "        config.experiment.project,\n",
    "        config=config_dict,  \n",
    "        init_kwargs={\"wandb\": {\"name\": config.experiment.name}},\n",
    "    )\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6da27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'mask_token_id': 58497} were passed to Showo, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 13, 16, 16) = 3328 dimensions.\n",
      "Look-up free quantizer with codebook size: 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention implementation:  sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\modeling_showo.py:49: FutureWarning: Accessing config attribute `w_clip_vit` directly via 'Showo' object attribute is deprecated. Please access 'w_clip_vit' over 'Showo's config object instead, e.g. 'unet.config.w_clip_vit'.\n",
      "  if self.w_clip_vit:\n",
      "All model checkpoint weights were used when initializing Showo.\n",
      "\n",
      "Some weights of Showo were not initialized from the model checkpoint at C:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\show-o-512x512-wo-llava-tuning and are newly initialized: ['mm_projector.2.weight', 'mm_projector.2.bias', 'mm_projector.0.bias', 'mm_projector.0.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,454,472,322\n",
      "VQ Model parameters: 95,387,004\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model.showo.llm_model_path,\n",
    "padding_side=\"left\")\n",
    "uni_prompting = UniversalPrompting(tokenizer,\n",
    "max_text_len=config.dataset.preprocessing.max_seq_length,\n",
    "                                special_tokens=(\"<|soi|>\", \"<|eoi|>\",\n",
    "\"<|sov|>\", \"<|eov|>\", \"<|t2i|>\", \"<|mmu|>\", \"<|t2v|>\", \"<|v2v|>\", \"<|lvg|>\"),\n",
    "                                ignore_id=-100,\n",
    "cond_dropout_prob=config.training.cond_dropout_prob)\n",
    "\n",
    "# Load VQ model\n",
    "def get_vq_model_class(model_type):\n",
    "    if model_type == \"magvitv2\":\n",
    "        return MAGVITv2\n",
    "    else:\n",
    "        raise ValueError(f\"model_type {model_type} not supported.\")\n",
    "\n",
    "vq_model_class = get_vq_model_class(config.model.vq_model.type)\n",
    "vq_model = vq_model_class.from_pretrained(config.model.vq_model.vq_model_name)\n",
    "vq_model.requires_grad_(False)\n",
    "vq_model.eval()\n",
    "\n",
    "# Load main model\n",
    "model = Showo.from_pretrained(config.model.showo.pretrained_model_path, **config.model.showo)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"VQ Model parameters: {sum(p.numel() for p in vq_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e805aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2I batch size: 2\n",
      "MMU batch size: 4\n",
      "Setting up Fashion Image Generation dataset...\n",
      "Found 12 tar files\n",
      "Total samples loaded: 120000\n",
      "FashionImageGenerationDataset loaded 120000 samples\n",
      "Fashion Image Generation dataset size: 120000\n",
      "Steps per epoch: 60000\n",
      "Estimated epochs: 1\n"
     ]
    }
   ],
   "source": [
    "# Setup dataset configurations\n",
    "dataset_config = config.dataset.params\n",
    "preproc_config = config.dataset.preprocessing\n",
    "\n",
    "# Calculate total batch sizes\n",
    "total_batch_size_t2i = config.training.batch_size_t2i * accelerator.num_processes * config.training.gradient_accumulation_steps\n",
    "total_batch_size_mmu = config.training.batch_size_mmu * accelerator.num_processes * config.training.gradient_accumulation_steps\n",
    "\n",
    "print(f\"T2I batch size: {total_batch_size_t2i}\")\n",
    "print(f\"MMU batch size: {total_batch_size_mmu}\")\n",
    "\n",
    "# Setup Fashion Image Generation Dataset (T2I flow)\n",
    "if config.dataset.gen_type == \"fashion_image_generation\":\n",
    "    print(\"Setting up Fashion Image Generation dataset...\")\n",
    "\n",
    "    dataset_fashion_img = FashionImageGenerationDataset(\n",
    "        data_root=dataset_config.train_t2i_shards_path_or_url,\n",
    "        split=\"train\"\n",
    "    )\n",
    "\n",
    "    print(f\"Fashion Image Generation dataset size: {len(dataset_fashion_img)}\")\n",
    "\n",
    "    # Create dataloader\n",
    "    if accelerator.num_processes > 1:\n",
    "        sampler = DistributedSampler(dataset_fashion_img,\n",
    "                                    num_replicas=accelerator.num_processes,\n",
    "                                    rank=accelerator.process_index,\n",
    "                                    shuffle=True)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    train_dataloader_t2i = DataLoader(\n",
    "        dataset_fashion_img,\n",
    "        batch_size=config.training.batch_size_t2i,\n",
    "        sampler=sampler,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=dataset_config.num_workers\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(len(dataset_fashion_img) / total_batch_size_t2i)\n",
    "    num_train_epochs = math.ceil(config.training.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    print(f\"Steps per epoch: {num_update_steps_per_epoch}\")\n",
    "    print(f\"Estimated epochs: {num_train_epochs}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported gen_type: {config.dataset.gen_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49844981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up FashionRec dataset...\n",
      "FashionRec dataset size: 303937\n",
      "MMU dataloader created with batch size: 4\n"
     ]
    }
   ],
   "source": [
    "# Setup FashionRec Dataset (MMU flow)\n",
    "if config.dataset.und_type == \"fashionrec\":\n",
    "    print(\"Setting up FashionRec dataset...\")\n",
    "\n",
    "    # Create dataset with only the parameters it expects\n",
    "    dataset_fashionrec = FashionRecDataset(\n",
    "        data_root=dataset_config.fashionrec_data_root,\n",
    "        split=\"train\",\n",
    "        task_weights=dataset_config.task_weights\n",
    "    )\n",
    "\n",
    "    print(f\"FashionRec dataset size: {len(dataset_fashionrec)}\")\n",
    "\n",
    "    # Create dataloader\n",
    "    if accelerator.num_processes > 1:\n",
    "        sampler_mmu = DistributedSampler(dataset_fashionrec,\n",
    "                                        num_replicas=accelerator.num_processes,\n",
    "                                        rank=accelerator.process_index,\n",
    "                                        shuffle=True)\n",
    "        shuffle_mmu = False\n",
    "    else:\n",
    "        sampler_mmu = None\n",
    "        shuffle_mmu = True\n",
    "\n",
    "    train_dataloader_mmu = DataLoader(\n",
    "        dataset_fashionrec,\n",
    "        batch_size=config.training.batch_size_mmu,\n",
    "        sampler=sampler_mmu,\n",
    "        shuffle=shuffle_mmu,\n",
    "        num_workers=dataset_config.num_workers\n",
    "        # Remove collate_fn for now - test if it's needed\n",
    "    )\n",
    "\n",
    "    print(f\"MMU dataloader created with batch size: {config.training.batch_size_mmu}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported und_type: {config.dataset.und_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f38224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing datasets...\n",
      "✅ T2I batch keys: dict_keys(['pixel_values', 'input_ids'])\n",
      "✅ T2I pixel_values shape: torch.Size([2, 3, 512, 512])\n",
      "✅ T2I pixel_values dtype: torch.float32\n",
      "✅ T2I input_ids type: <class 'list'>\n",
      "✅ T2I input_ids length: 2\n",
      "✅ T2I first description: A loafer. The shoes are made of dark blue velvet with gold embroidery at the top, featuring a slip-o...\n",
      "✅ MMU batch keys: dict_keys(['pixel_values', 'input_ids', 'task'])\n",
      "✅ MMU pixel_values shape: torch.Size([4, 3, 512, 512])\n",
      "✅ MMU pixel_values dtype: torch.float32\n",
      "✅ MMU input_ids type: <class 'list'>\n",
      "✅ MMU input_ids length: 4\n",
      "✅ MMU tasks: ['basic_recommendation', 'personalized_recommendation', 'personalized_recommendation', 'alternative_recommendation']\n",
      "✅ MMU first conversation: human: I uploaded a picture of my outfit. What kind of jeans would go well with my plaid wool coat a...\n",
      "\n",
      "Dataset setup complete! ✅\n"
     ]
    }
   ],
   "source": [
    "# Test both datasets with small batches\n",
    "print(\"Testing datasets...\")\n",
    "\n",
    "# Test T2I dataset\n",
    "try:\n",
    "    t2i_batch = next(iter(train_dataloader_t2i))\n",
    "    print(f\"T2I batch keys: {t2i_batch.keys()}\")\n",
    "    print(f\"T2I pixel_values shape: {t2i_batch['pixel_values'].shape}\")\n",
    "    print(f\"T2I pixel_values dtype: {t2i_batch['pixel_values'].dtype}\")\n",
    "    print(f\"T2I input_ids type: {type(t2i_batch['input_ids'])}\")  \n",
    "    print(f\"T2I input_ids length: {len(t2i_batch['input_ids'])}\")  \n",
    "    print(f\"T2I first description: {t2i_batch['input_ids'][0][:100]}...\") \n",
    "except Exception as e:\n",
    "    print(f\"T2I dataset error: {e}\")\n",
    "\n",
    "# Test MMU dataset\n",
    "try:\n",
    "    mmu_batch = next(iter(train_dataloader_mmu))\n",
    "    print(f\"MMU batch keys: {mmu_batch.keys()}\")\n",
    "    print(f\"MMU pixel_values shape: {mmu_batch['pixel_values'].shape}\")\n",
    "    print(f\"MMU pixel_values dtype: {mmu_batch['pixel_values'].dtype}\")\n",
    "    print(f\"MMU input_ids type: {type(mmu_batch['input_ids'])}\") \n",
    "    print(f\"MMU input_ids length: {len(mmu_batch['input_ids'])}\")\n",
    "    print(f\"MMU tasks: {mmu_batch['task']}\")  # List of task names\n",
    "    print(f\"MMU first conversation: {mmu_batch['input_ids'][0][:100]}...\") \n",
    "except Exception as e:\n",
    "    print(f\"MMU dataset error: {e}\")\n",
    "\n",
    "print(\"\\nDataset setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0f4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW\n",
      "LR Scheduler: constant_with_warmup\n",
      "Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer and scheduler\n",
    "from torch.optim import AdamW\n",
    "from models.lr_schedulers import get_scheduler\n",
    "\n",
    "# Prepare model for training\n",
    "if config.model.gradient_checkpointing:\n",
    "    model._set_gradient_checkpointing(model, value=True)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.optimizer.params.learning_rate,\n",
    "    weight_decay=config.optimizer.params.weight_decay,\n",
    "    betas=(config.optimizer.params.beta1, config.optimizer.params.beta2),\n",
    "    eps=config.optimizer.params.epsilon\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    config.lr_scheduler.scheduler,\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.lr_scheduler.params.warmup_steps,\n",
    "    num_training_steps=config.training.max_train_steps\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: {type(optimizer).__name__}\")\n",
    "print(f\"LR Scheduler: {config.lr_scheduler.scheduler}\")\n",
    "print(f\"Learning rate: {config.optimizer.params.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6cd1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing components with accelerator...\n",
      "Model on device: cuda\n",
      "VQ model on device: cuda:0\n",
      "Mixed precision: fp16\n",
      "Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Prepare model, optimizer, scheduler, and dataloaders with accelerator\n",
    "print(\"Preparing components with accelerator...\")\n",
    "\n",
    "# Prepare everything for distributed training\n",
    "model, optimizer, lr_scheduler, train_dataloader_t2i, train_dataloader_mmu = accelerator.prepare(model, optimizer, lr_scheduler, train_dataloader_t2i, train_dataloader_mmu)\n",
    "\n",
    "# Move VQ model to the same device\n",
    "vq_model = vq_model.to(accelerator.device)\n",
    "\n",
    "print(f\"Model on device: {accelerator.device}\")\n",
    "print(f\"VQ model on device: {next(vq_model.parameters()).device}\")\n",
    "print(f\"Mixed precision: {accelerator.mixed_precision}\")\n",
    "print(f\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ff70b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "Dataset sizes: T2I=120000, MMU=303937\n",
      "Steps per epoch: 60000\n",
      "Max training steps: 50000\n",
      "Estimated epochs: 1\n",
      "Gradient accumulation: 1\n",
      "Training setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop setup\n",
    "from training.utils import AverageMeter\n",
    "import time\n",
    "\n",
    "# Calculate training parameters\n",
    "num_update_steps_per_epoch = math.ceil(len(dataset_fashion_img) / (config.training.batch_size_t2i * accelerator.num_processes))\n",
    "max_train_steps = config.training.max_train_steps\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"Dataset sizes: T2I={len(dataset_fashion_img)}, MMU={len(dataset_fashionrec)}\")\n",
    "print(f\"Steps per epoch: {num_update_steps_per_epoch}\")\n",
    "print(f\"Max training steps: {max_train_steps}\")\n",
    "print(f\"Estimated epochs: {num_train_epochs}\")\n",
    "print(f\"Gradient accumulation: {config.training.gradient_accumulation_steps}\")\n",
    "\n",
    "# Initialize metrics tracking\n",
    "loss_meter_t2i = AverageMeter()\n",
    "loss_meter_mmu = AverageMeter()\n",
    "loss_meter_total = AverageMeter()\n",
    "\n",
    "# Set models to training mode\n",
    "model.train()\n",
    "vq_model.eval()  # VQ model stays in eval mode\n",
    "\n",
    "print(f\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b4e659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single training step...\n",
      "T2I batch: pixel_values torch.Size([2, 3, 512, 512]), 2 texts\n",
      "MMU batch: pixel_values torch.Size([4, 3, 512, 512]), 4 conversations\n",
      "Single step test failed: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_40340\\3826167698.py\", line 16, in <module>\n",
      "    t2i_image_tokens = vq_model.get_code(t2i_images)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\modeling_magvitv2.py\", line 424, in get_code\n",
      "    hidden_states = self.encoder(pixel_values)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\modeling_magvitv2.py\", line 151, in forward\n",
      "    h = self.down[i_level].block[i_block](hs[-1], temb)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\common_modules.py\", line 340, in forward\n",
      "    h = nonlinearity(h)\n",
      "        ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\common_modules.py\", line 18, in nonlinearity\n",
      "    return x * torch.sigmoid(x)\n",
      "           ~~^~~~~~~~~~~~~~~~~~\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# Test a single training step to make sure everything works\n",
    "print(\"Testing single training step...\")\n",
    "\n",
    "try:\n",
    "    # Get single batches from both dataloaders\n",
    "    t2i_batch = next(iter(train_dataloader_t2i))\n",
    "    mmu_batch = next(iter(train_dataloader_mmu))\n",
    "\n",
    "    print(f\"T2I batch: pixel_values {t2i_batch['pixel_values'].shape}, {len(t2i_batch['input_ids'])} texts\")\n",
    "    print(f\"MMU batch: pixel_values {mmu_batch['pixel_values'].shape}, {len(mmu_batch['input_ids'])} conversations\")\n",
    "\n",
    "    # Test tokenization and processing\n",
    "    with torch.no_grad():\n",
    "        # Process T2I batch\n",
    "        t2i_images = t2i_batch['pixel_values']\n",
    "        t2i_image_tokens = vq_model.get_code(t2i_images)\n",
    "        t2i_image_tokens = t2i_image_tokens + len(uni_prompting.text_tokenizer)\n",
    "\n",
    "        print(f\"T2I image tokens shape: {t2i_image_tokens.shape}\")\n",
    "        print(f\"T2I image tokens range: {t2i_image_tokens.min().item()} to {t2i_image_tokens.max().item()}\")\n",
    "\n",
    "        # Process MMU batch\n",
    "        mmu_images = mmu_batch['pixel_values']\n",
    "        mmu_image_tokens = vq_model.get_code(mmu_images)\n",
    "        mmu_image_tokens = mmu_image_tokens + len(uni_prompting.text_tokenizer)\n",
    "\n",
    "        print(f\"MMU image tokens shape: {mmu_image_tokens.shape}\")\n",
    "        print(f\"MMU image tokens range: {mmu_image_tokens.min().item()} to {mmu_image_tokens.max().item()}\")\n",
    "\n",
    "    print(\"Single step test passed! Ready for training loop.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Single step test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_312 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
