{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import Showo, MAGVITv2\n",
    "from training.prompting_utils import UniversalPrompting, create_attention_mask_for_mmu, create_attention_mask_for_mmu_vit\n",
    "from training.utils import get_config, flatten_omega_conf, image_transform\n",
    "from transformers import AutoTokenizer\n",
    "from models.clip_encoder import CLIPVisionTower\n",
    "from transformers import CLIPImageProcessor\n",
    "import llava.llava.conversation as conversation_lib\n",
    "\n",
    "conversation_lib.default_conversation = conversation_lib.conv_templates[\"phi1.5\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import os\n",
    "\n",
    "config = OmegaConf.load('configs/fashionm3_training.yaml')\n",
    "\n",
    "# Fix paths for Windows\n",
    "base_path = os.getcwd()  # Current directory\n",
    "config.model.vq_model.pretrained_model_path = os.path.join(base_path, \"models\",\n",
    "\"magvitv2\", \"pytorch_model.safetensors\")\n",
    "config.model.showo.pretrained_model_path = os.path.join(base_path, \"models\",\n",
    "\"show-o-512x512-wo-llava-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "GPU Memory: 8.0GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory /\n",
    "1024**3:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show o tokenizer setup and adding special tokens to universal prompting\n",
    "# llm model : 'microsoft/phi-1_5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model.showo.llm_model_path, padding_side =\"left\")\n",
    "uni_prompting = UniversalPrompting(tokenizer, max_text_len=config.dataset.preprocessing.max_seq_length,\n",
    "                                       special_tokens=(\"<|soi|>\", \"<|eoi|>\", \"<|sov|>\", \"<|eov|>\", \"<|t2i|>\", \"<|mmu|>\", \"<|t2v|>\", \"<|v2v|>\", \"<|lvg|>\"),\n",
    "                                       ignore_id=-100, cond_dropout_prob=config.training.cond_dropout_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\magvitv2\\pytorch_model.safetensors\n",
      "File exists: True\n",
      "File size: 0.36 GB\n",
      "First 20 bytes: b'x\\x9a\\x00\\x00\\x00\\x00\\x00\\x00{\"__metadata'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "magvit_path = config.model.vq_model.pretrained_model_path\n",
    "print(f\"File path: {magvit_path}\")\n",
    "print(f\"File exists: {os.path.exists(magvit_path)}\")\n",
    "if os.path.exists(magvit_path):\n",
    "    file_size = os.path.getsize(magvit_path) / (1024**3)  # GB\n",
    "    print(f\"File size: {file_size:.2f} GB\")\n",
    "\n",
    "    # Try to peek at the file content\n",
    "    with open(magvit_path, 'rb') as f:\n",
    "        first_bytes = f.read(20)\n",
    "        print(f\"First 20 bytes: {first_bytes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MAGVIT-v2 from local path...\n",
      "Working with z of shape (1, 13, 16, 16) = 3328 dimensions.\n",
      "Look-up free quantizer with codebook size: 8192\n",
      "Loading from: c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\magvitv2\\pytorch_model.safetensors\n",
      "MAGVIT-v2 loaded with safetensors\n",
      "MAGVIT-v2 parameters: 95,387,004\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading MAGVIT-v2 from local path...\")\n",
    "\n",
    "try:\n",
    "    # Try safetensors loading first\n",
    "    from safetensors.torch import load_file\n",
    "\n",
    "    vq_model = MAGVITv2().to(device)\n",
    "    print(f\"Loading from: {config.model.vq_model.pretrained_model_path}\")\n",
    "\n",
    "    # Load with safetensors\n",
    "    state_dict = load_file(config.model.vq_model.pretrained_model_path)\n",
    "\n",
    "    # The state_dict might need to be wrapped in a 'model' key or used directly\n",
    "    if 'model' in state_dict:\n",
    "        vq_model.load_state_dict(state_dict['model'])\n",
    "    else:\n",
    "        vq_model.load_state_dict(state_dict)\n",
    "\n",
    "    print(\"MAGVIT-v2 loaded with safetensors\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Safetensors loading failed: {e}\")\n",
    "    print(\"Trying HuggingFace fallback...\")\n",
    "\n",
    "    # Fallback to HuggingFace\n",
    "    vq_model = MAGVITv2.from_pretrained(config.model.vq_model.vq_model_name).to(device)\n",
    "    print(\"MAGVIT-v2 loaded from HuggingFace\")\n",
    "\n",
    "vq_model.requires_grad_(False)\n",
    "vq_model.eval()\n",
    "print(f\"MAGVIT-v2 parameters: {sum(p.numel() for p in\n",
    "vq_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# setting up vision tower: clip-vit\n",
    "vision_tower_name =\"openai/clip-vit-large-patch14-336\"\n",
    "vision_tower = CLIPVisionTower(vision_tower_name).to(device)\n",
    "clip_image_processor = CLIPImageProcessor.from_pretrained(vision_tower_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Showo.from_pretrained(\"showlab/show-o\")\n",
    "#model = model.to(device)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'mask_token_id': 58497} were passed to Showo, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention implementation:  sdpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\modeling_showo.py:49: FutureWarning: Accessing config attribute `w_clip_vit` directly via 'Showo' object attribute is deprecated. Please access 'w_clip_vit' over 'Showo's config object instead, e.g. 'unet.config.w_clip_vit'.\n",
      "  if self.w_clip_vit:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Showo does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# setting up the showo model \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mShowo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretrained_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_clip_vit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      3\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\modeling_utils.py:772\u001b[39m, in \u001b[36mModelMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# else let accelerate handle loading and dispatching.\u001b[39;00m\n\u001b[32m    769\u001b[39m     \u001b[38;5;66;03m# Load weights and dispatch according to the device_map\u001b[39;00m\n\u001b[32m    770\u001b[39m     \u001b[38;5;66;03m# by default the device_map is None and the weights are loaded on the CPU\u001b[39;00m\n\u001b[32m    771\u001b[39m     force_hook = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m     device_map = \u001b[43m_determine_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_sharded:\n\u001b[32m    774\u001b[39m         \u001b[38;5;66;03m# we load the parameters on the cpu\u001b[39;00m\n\u001b[32m    775\u001b[39m         device_map = {\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonat\\Desktop\\StyleAI\\venv_312\\Lib\\site-packages\\diffusers\\models\\model_loading_utils.py:58\u001b[39m, in \u001b[36m_determine_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, torch_dtype)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_determine_device_map\u001b[39m(model: torch.nn.Module, device_map, max_memory, torch_dtype):\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         no_split_modules = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_no_split_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m         device_map_kwargs = {\u001b[33m\"\u001b[39m\u001b[33mno_split_module_classes\u001b[39m\u001b[33m\"\u001b[39m: no_split_modules}\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m device_map != \u001b[33m\"\u001b[39m\u001b[33msequential\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonat\\Desktop\\StyleAI\\backend\\fashionm3\\Show-o\\models\\modeling_utils.py:1000\u001b[39m, in \u001b[36mModelMixin._get_no_split_modules\u001b[39m\u001b[34m(self, device_map)\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, ModelMixin):\n\u001b[32m    999\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._no_split_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1000\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1001\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not support `device_map=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_map\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`. To implement support, the model \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1002\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mclass needs to implement the `_no_split_modules` attribute.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1003\u001b[39m         )\n\u001b[32m   1004\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1005\u001b[39m         _no_split_modules = _no_split_modules | \u001b[38;5;28mset\u001b[39m(module._no_split_modules)\n",
      "\u001b[31mValueError\u001b[39m: Showo does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute."
     ]
    }
   ],
   "source": [
    "# setting up the showo model \n",
    "model = Showo.from_pretrained(config.model.showo.pretrained_model_path, w_clip_vit=True,  low_cpu_mem_usage=True, device_map = \"auto\", torch_dtype=torch.float32).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the parameters\n",
    "temperature = 0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 1  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "SYSTEM_PROMPT = \"A chat between a curious user and an artificial intelligence assistant. \" \\\n",
    "                \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "SYSTEM_PROMPT_LEN = 28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "# Available fashion images (extracted from our training data)\n",
    "fashion_images = {\n",
    "    \"black_trousers\": \"./0000000.jpg\",\n",
    "    \"pink_jacket\": \"./0000001.jpg\",\n",
    "    \"fashion_item_3\": \"./0000002.jpg\"\n",
    "}\n",
    "\n",
    "# Choose which image to test\n",
    "selected_image = \"pink_jacket\"  # or \"black_trousers\"\n",
    "image_path = fashion_images[selected_image]\n",
    "\n",
    "print(f\"Testing with: {selected_image}\")\n",
    "print(f\"Path: {image_path}\")\n",
    "\n",
    "# Display the image\n",
    "if os.path.exists(image_path):\n",
    "    display(Image(filename=image_path))\n",
    "    print(f\"Fashion image loaded: {selected_image}\")\n",
    "else:\n",
    "    print(f\"Image not found: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "from PIL import Image\n",
    "## arguments\n",
    "input_image_path = fashion_images[selected_image]\n",
    "questions ='Please describe this image in detail. *** What style elements do you notice? *** What fashion advice would you give?'\n",
    "\n",
    "## processing\n",
    "questions = questions.split('***')\n",
    "image_ori = Image.open(input_image_path).convert(\"RGB\")\n",
    "# tranforming the image to the required resolution:256x256\n",
    "image = image_transform(image_ori, resolution = config.dataset.preprocessing.resolution).to(device)\n",
    "image = image.unsqueeze(0)\n",
    "print(f\"image shape: {image.shape}\") # torch.Size([1, 3, 256, 256])\n",
    "pixel_values = clip_image_processor.preprocess(image_ori,return_tensors=\"pt\")['pixel_values'][0]\n",
    "print(f\"pixel values shape: {pixel_values.shape}\")\n",
    "image_tokens = vq_model.get_code(image) + len(uni_prompting.text_tokenizer)\n",
    "print(f\"image tokens shape: {image_tokens.shape}\") # torch.Size([1, 256])\n",
    "batch_size = 1\n",
    "\n",
    "## inference\n",
    "for question in questions: \n",
    "  conv = conversation_lib.default_conversation.copy()\n",
    "  print(f\"conversation: {conv}\")\n",
    "  conv.append_message(conv.roles[0], question)\n",
    "  conv.append_message(conv.roles[1], None)\n",
    "  prompt_question = conv.get_prompt()\n",
    "  # print(prompt_question)\n",
    "  question_input = []\n",
    "  question_input.append(prompt_question.strip())\n",
    "  print(f\"system prompt: {SYSTEM_PROMPT}\")\n",
    "  input_ids_system = [uni_prompting.text_tokenizer(SYSTEM_PROMPT, return_tensors=\"pt\", padding=\"longest\").input_ids for _ in range(batch_size)]\n",
    "  print(f\"system prompt input ids: {input_ids_system}\")\n",
    "  input_ids_system = torch.stack(input_ids_system, dim=0)\n",
    "  assert input_ids_system.shape[-1] == 28\n",
    "  print(f\"after torch stacking: {input_ids_system}\")\n",
    "  input_ids_system = input_ids_system.clone().detach().to(device)\n",
    "  # inputs_ids_system = input_ids_system.to(device)\n",
    "#   inputs_ids_system = torch.tensor(input_ids_system).to(device).squeeze(0)\n",
    "  \n",
    "  print(f\"after moving to device: {input_ids_system}\")\n",
    "  input_ids_system = input_ids_system[0]\n",
    "  print(f\"after indexing 0: {input_ids_system}\")\n",
    "  \n",
    "  \n",
    "  print(f\"question input: {question_input}\")\n",
    "  input_ids = [uni_prompting.text_tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").input_ids for prompt in question_input]\n",
    "  print(f\"after tokenizing the question: {input_ids}\")\n",
    "  input_ids = torch.stack(input_ids)\n",
    "  print(f\"after torch stacking: {input_ids}\")\n",
    "  input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "                        input_ids, batch_first=True, padding_value=uni_prompting.text_tokenizer.pad_token_id\n",
    "                )\n",
    "  print(f\"after padding: {input_ids}\")\n",
    "  # input_ids = torch.tensor(input_ids).to(device).squeeze(0)\n",
    "  input_ids = input_ids.clone().detach().to(device).squeeze(0)\n",
    "  print(f\"after moving to device: {input_ids}\")\n",
    "  input_ids_llava = torch.cat([\n",
    "                          (torch.ones(input_ids.shape[0], 1) *uni_prompting.sptids_dict['<|mmu|>']).to(device),\n",
    "                          input_ids_system,\n",
    "                          (torch.ones(input_ids.shape[0], 1) * uni_prompting.sptids_dict['<|soi|>']).to(device),\n",
    "                          # place your img embedding here\n",
    "                          (torch.ones(input_ids.shape[0], 1) * uni_prompting.sptids_dict['<|eoi|>']).to(device),\n",
    "                          input_ids,\n",
    "                  ], dim=1).long()\n",
    "  print(input_ids_llava)\n",
    "  \n",
    "  images_embeddings = vision_tower(pixel_values[None])\n",
    "  print(f\"images embeddings shape: {images_embeddings.shape}\")# torch.Size([1, 576, 1024])\n",
    "  images_embeddings = model.mm_projector(images_embeddings)\n",
    "  print(f\"images embeddings shape after projection: {images_embeddings.shape}\") \n",
    "\n",
    "  text_embeddings = model.showo.model.embed_tokens(input_ids_llava)\n",
    "\n",
    "  #full input seq\n",
    "  part1 = text_embeddings[:, :2+SYSTEM_PROMPT_LEN,:]\n",
    "  part2 = text_embeddings[:, 2+SYSTEM_PROMPT_LEN:,:]\n",
    "  input_embeddings = torch.cat((part1,images_embeddings,part2),dim=1)\n",
    "\n",
    "  attention_mask_llava = create_attention_mask_for_mmu_vit(input_embeddings,system_prompt_len=SYSTEM_PROMPT_LEN)\n",
    "\n",
    "  cont_toks_list = model.mmu_generate(\n",
    "    input_embeddings = input_embeddings,\n",
    "    attention_mask = attention_mask_llava[0].unsqueeze(0),\n",
    "    max_new_tokens = 100,\n",
    "    top_k = top_k,\n",
    "    eot_token = uni_prompting.sptids_dict['<|eov|>']\n",
    "  )\n",
    "  \n",
    "  cont_toks_list = torch.stack(cont_toks_list).squeeze()[None]\n",
    "  text = uni_prompting.text_tokenizer.batch_decode(cont_toks_list,skip_special_tokens=True)\n",
    "  print(f\"User: {question}, \\nAnswer: {text[0]}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_312 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
